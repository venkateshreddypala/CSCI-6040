{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "How to Prepare Text Data With Keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJQewzyyJivw",
        "colab_type": "text"
      },
      "source": [
        "# How to Prepare Text Data With Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD2JGO_xJivz",
        "colab_type": "text"
      },
      "source": [
        "As we cannot Feed the raw text directly into deep learning models,the text data must be encoded as numbers to be used asa input or output for machine learning and deep learning models.As with any neural network, we need to convert our data into a numeric format; in Keras and TensorFlow we work with tensors. The IMDB example data from the keras package has been preprocessed to a list of integers, where every integer corresponds to a word arranged by descending word frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rDoR8BfJiv0",
        "colab_type": "text"
      },
      "source": [
        "# 7.1 Split Words with text_to_word_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA7vIRYKJiv0",
        "colab_type": "text"
      },
      "source": [
        "Words are called tokens and the process of splitting the text into tokens is called tokenization  in that the first step is when working with text is to spliting it into the words,As the keras provides the text_to_word_sequece() function that is been used to split the text into a list of words. This function will automatically does three things \n",
        " 1. Split Words by Space\n",
        " 2. Filters out Punctuation\n",
        " 3. Converts text it lowercas(lower=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmxUjx8EJiv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "#define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "#tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yjn8yEGJiv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBH5g7jXJiv7",
        "colab_type": "text"
      },
      "source": [
        "# 7.2 Encoding with one hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWUwPmdJiv8",
        "colab_type": "text"
      },
      "source": [
        "A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f35njwwjJiv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "#define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "#define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U22FIzRJiv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "#define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "#estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "#integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPvkTukJiwA",
        "colab_type": "text"
      },
      "source": [
        "8\n",
        "[5, 9, 8, 7, 9, 1, 5, 3, 8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP0Dvl9cJiwB",
        "colab_type": "text"
      },
      "source": [
        "# 7.3 Hash Encoding with hashing_trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtsONyi5JiwC",
        "colab_type": "text"
      },
      "source": [
        "The choice of hash function determines the range of possible outputs, i.e. the range is always fixed (e.g. numbers from 0 to 1024). Hash functions are one-way: given a hash, we can't perform a reverse lookup to determine what the input was. Hash functions may output the same value for different inputs (collision).The limitatio of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers.This avoids the need of keeping a track if the vocabulary which is faster and requires less memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PzWMmR5JiwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import hashing_trick\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "#define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "#estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "#integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGsFaZ-EJiwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "8\n",
        "[6, 4, 1, 2, 7, 5, 6, 2, 6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TgqNO6IJiwH",
        "colab_type": "text"
      },
      "source": [
        "# 7.4 Tokenizer API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY_p7ddUJiwI",
        "colab_type": "text"
      },
      "source": [
        "Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents,as the keras providesa the tokenizer class for preparing text documents for deep learning,this may be preffered approach for large projects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uvgxEyNJiwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "#define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "#create the tokenizer\n",
        "t = Tokenizer()\n",
        "#fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScEywYQFJiwM",
        "colab_type": "text"
      },
      "source": [
        "word_couts : A dictionary of words and their counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeal3yOOJiwN",
        "colab_type": "text"
      },
      "source": [
        "word_docs : An integer count of the total number of documents that were used to fit the Tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VhOFQYBJiwO",
        "colab_type": "text"
      },
      "source": [
        "word index: A dictionary of words and their uniquely assigned integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qE4axfFJiwO",
        "colab_type": "text"
      },
      "source": [
        "document count: A dictionary of words and how many documents each appeared in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ3jI8lfJiwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkqKTMoIJiwR",
        "colab_type": "text"
      },
      "source": [
        "binary: Whether or not each word is present in the document. This is the default.\n",
        "\n",
        "count: The count of each word in the document.\n",
        "\n",
        "tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "\n",
        "freq: The frequency of each word as a ratio of words within each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM28Zp_tJiwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "#define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "#create the tokenizer\n",
        "t = Tokenizer()\n",
        "#fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "#summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "#integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}