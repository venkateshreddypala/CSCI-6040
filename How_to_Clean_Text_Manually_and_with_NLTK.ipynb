{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "How to Clean Text Manually and with NLTK.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlMrXt08Ic9W",
        "colab_type": "text"
      },
      "source": [
        "# How to Clean Text Manually and with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CUUS7xPIc9Y",
        "colab_type": "text"
      },
      "source": [
        "We Cannot from raw text to fitting a machine leraning and deep learning model, at first we must clear the text first which means spliting it into words by handling the puncuations and cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SIE5o2fIc9Z",
        "colab_type": "text"
      },
      "source": [
        "This whole part was divided into six parts, those are\n",
        "\n",
        "1. Metamorphosis by Franz Kafka\n",
        "\n",
        "2. Text Cleaning is Task Specific\n",
        "\n",
        "3. Manual Tokenization\n",
        "\n",
        "4. Tokenization and Cleaning with NLTK\n",
        "\n",
        "5. Additional Text Cleaning Considerations\n",
        "\n",
        "6. Tips for Cleaning Text for Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQhWumcrIc9a",
        "colab_type": "text"
      },
      "source": [
        "# 5.1 Metamorphosis By Franz Kafka"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Tm7O2bIc9a",
        "colab_type": "text"
      },
      "source": [
        "Let's Start off by selecting a dataset. Using the text from the book of metamorphosis by franz kafka, there is no specific reason for using it because it is short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYq5bRm2Ic9b",
        "colab_type": "text"
      },
      "source": [
        "# 5.2 Text Cleaning Is Task Specific"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWS2hj4KIc9c",
        "colab_type": "text"
      },
      "source": [
        "Getting hold of the Text data, the first step in cleaning up thr trxt data is to have a strong idea about what we are upto, and in the context of review the text we have type is to see what excatly might help. Taking a look into the text, What do you notice ? I see\n",
        "\n",
        "It's plain text so there is no markup to parse (yay!).\n",
        "\n",
        "The translation of the original German uses UK English (e.g. travelling).\n",
        "\n",
        "The lines are arti\fcially wrapped with new lines at about 70 characters (meh).\n",
        "\n",
        "There are no obvious typos or spelling mistakes.\n",
        "\n",
        "There's punctuation like commas, apostrophes, quotes, question marks, and more.\n",
        "\n",
        "There's hyphenated descriptions like armour-like.\n",
        "\n",
        "There's a lot of use of the em dash (-) to continue sentences (maybe replace with commas?).\n",
        "\n",
        "There are names (e.g. Mr. Samsa)\n",
        "\n",
        "I'm sure that there is lot more going on to the trained eye, as we are going to look at the general text cleaning steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui5bY2NEIc9d",
        "colab_type": "text"
      },
      "source": [
        "# 5.3 Manual Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMJj1AfiIc9d",
        "colab_type": "text"
      },
      "source": [
        "The text cleaning is hard,but the text that has choosen to work with is pretty clean already. We cloud just write some python code to clean it up manually,and it is a good exercise for the simple problems that you encounter tools like regular expressions and splitting strings can get into a long way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvmbbpfLIc9e",
        "colab_type": "text"
      },
      "source": [
        "# 5.4 Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fBYONmhIc9e",
        "colab_type": "text"
      },
      "source": [
        "Loading the text Data so that we can work with it ,as the text is small and will load quicklu and it is easily fit into memory as this was ot always be the case and we may need to write code to memory map the file.NLTK is a tool that will make working with large files much easier. As we can load the entire metamorphosis_clean.txt in the memory as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo-e0BjiIc9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load text\n",
        "filename ='metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVxMq-LOIc9i",
        "colab_type": "text"
      },
      "source": [
        "# 5.4.1 Split By Whitespace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh-1s8UvIc9i",
        "colab_type": "text"
      },
      "source": [
        "The clean text often means a list of words or tokens that we can work with in the achine learning models, that means converting the raw text into a list of words and saving it again.It was a simple way to do this would br to split the whole document by the white space, including \" \" (space),as the new lines tabs and more. As we can do this python with the split() function on the loaded string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs7ct4l9Ic9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load text\n",
        "filename = 'metamorphosis_claen.txt'\n",
        "file = open(filename,'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into words by white space\n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnmLB_1rIc9l",
        "colab_type": "text"
      },
      "source": [
        "As the example splits the document into a long list of words and prints the first 100,as we can see the puncuation is preserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvdKx9jgIc9m",
        "colab_type": "text"
      },
      "source": [
        "# 5.4.2 Select Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8XE8Xb-Ic9n",
        "colab_type": "text"
      },
      "source": [
        "Selecting Words has an another approach that might be using the regex model(re) and split the document into owrds by selecting for strings of alphanumeric characters(a-z,A-Z,0-9 and '_')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2XRKV10Ic9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "#load text \n",
        "filename = 'metamorphosis_clean.txt'\n",
        "flie = open(filename,'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split based on words only\n",
        "words = re.split(r'\\W+',text)\n",
        "print(words[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXT6X8A4Ic9p",
        "colab_type": "text"
      },
      "source": [
        "# 5.4.4 Split By Whitespace and Remove Punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aptrIF6DIc9q",
        "colab_type": "text"
      },
      "source": [
        "As the words,but without the punctuation like commas and quotes. We also want to keep the contractions together, by spliting it in one way for the document into words by white space and using the string translation to replace all the punctuation with nothing.As the pyhton provides a constat called string.puncutation that provides a greatlist of puncutation characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pOogwN1Ic9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Print(string.puncutation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij6T_E2tIc9u",
        "colab_type": "text"
      },
      "source": [
        "As we can also use the regular expression to select for the punctation characters and use the sub() function to replace with nothing in place those. For example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6NwGA55Ic9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re_punc = re.compile('[%s]'% re.escape(string.punctuation))\n",
        "#remove punctutation from each word \n",
        "stripped = [re_punc.sub('',w)for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TUeYzEgIc9w",
        "colab_type": "text"
      },
      "source": [
        "As we can put all of this together,load the text file,split it into words by white space, then translate each word to remove the puncutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEGPn0TDIc9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "#load text\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into words by white space\n",
        "words = text.split()\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTDDzS3DIc9z",
        "colab_type": "text"
      },
      "source": [
        "As sometimes the text data may contain the non-printable characters, so that we can use a similar approach to filter out all non-printable characters by selectig the inverse of the string.printable constant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bobiebzDIc9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "result = [re_print.sub('', w) for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnWcd_IIc91",
        "colab_type": "text"
      },
      "source": [
        "# 5.4.5 Normalizing Case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjIXrhspIc91",
        "colab_type": "text"
      },
      "source": [
        "As it was common to convert all the words into one case, means that the vocabulary will shrink in size as some of the distinctions will be lost,though we can convert all words to lowercase by callig the lower() function on each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSnnMUUGIc92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into words by white space\n",
        "words = text.split()\n",
        "#convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JswDtdY6Ic93",
        "colab_type": "text"
      },
      "source": [
        "# 5.5 Tokenization And Cleaning With NLTK "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhsBOiaZIc94",
        "colab_type": "text"
      },
      "source": [
        "As NLTK is called as the Natural Language Toolkit which is a python library written for working and modeling text, as it provides a good tool for loading and cleaning text that we can use to get the data ready for working with machine learning and deep learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANBzFxHoIc94",
        "colab_type": "text"
      },
      "source": [
        "# 5.5.1 Installing NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU4jFsGnIc95",
        "colab_type": "text"
      },
      "source": [
        "As we can install the NLTK from the package manager such as pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK5vMa15Ic96",
        "colab_type": "text"
      },
      "source": [
        "sudo pip install -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzaAUSUEIc96",
        "colab_type": "text"
      },
      "source": [
        "Later on installing the nltk we need to install the data used with the library,including the a great set of documents that we can use later for testing other tools in NLTK. There are few ways to do this, such as from within a script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-a8XenTIc97",
        "colab_type": "code",
        "colab": {},
        "outputId": "19dc070b-0703-4837-a526-b6fc187aca9f"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnLpk-j-Ic9_",
        "colab_type": "text"
      },
      "source": [
        "In other way by the command line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASnJzKKvIc9_",
        "colab_type": "code",
        "colab": {},
        "outputId": "f374bc06-6684-46c4-8030-d279bf06690f"
      },
      "source": [
        "python -m nltk.downloader all"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-a98312d574ff>, line 1)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-a98312d574ff>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m nltk.downloader all\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwD_gg2rIc-C",
        "colab_type": "text"
      },
      "source": [
        "# 5.5.2 Split Into Sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n0L-qAmIc-C",
        "colab_type": "text"
      },
      "source": [
        "Spliting the sentences if the text is a first useful step as some of the modeling tasks prefer input to be in the form of a paragraphs or senteces such as Word2Vec,as we could not split the text into sentences,spliting the each sentence into owrds as we save each sentence to file one per line as the NLTK provides the sent_tokenize() function to split text into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwAa0mcaIc-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fom nltk import sent_tokenize\n",
        "#load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "flie = open(filename,'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into sentences \n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp0-HvfjIc-G",
        "colab_type": "text"
      },
      "source": [
        "# 5.5.3 Split into Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhPLqqmMIc-G",
        "colab_type": "text"
      },
      "source": [
        "NLTK provides a fuction called word_tokenize() for splitting the strings into tokens,as it splits tokens based on white space and punctuation as well the commas and periods are taken as separate tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqfri7t8Ic-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "#load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into words\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8P0M_zdIc-J",
        "colab_type": "text"
      },
      "source": [
        "# 5.5.4 Filter Out Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFGGva_Ic-K",
        "colab_type": "text"
      },
      "source": [
        "The words that are not contributed to the deeper meaning of the phrase are called as Stop words, the common stop words are 'the','a','and','is'.The documentation classification may make sense to remove the stop words,as the NLTK provides a list of commonly agreed upon stop words for a variety of languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMSrlb_VIc-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLYVe4BXIc-M",
        "colab_type": "text"
      },
      "source": [
        "As the above line of code is used to remove all the lower case and have punctuation removed,we could compare your tokens to stop the words and filter them out\n",
        "\n",
        "load the raw text\n",
        "\n",
        "Spilt into tokens\n",
        "\n",
        "Convert to lowercase\n",
        "\n",
        "Remove punctuation from each token\n",
        "\n",
        "Filter out remaining tokens that are not alphabetic\n",
        "\n",
        "Filter out tokens that are stope words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL_S8y30Ic-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into words\n",
        "tokens = word_tokenize(text)\n",
        "#convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in tokens]\n",
        "#remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "#filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhiXo7A2Ic-P",
        "colab_type": "text"
      },
      "source": [
        "# 5.5.5 Stem Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grCzPkVDIc-P",
        "colab_type": "text"
      },
      "source": [
        "Stemming is one of the word that refers to reducing each word to its root or base,for some applications like document classification are benefit for the stemming in order to reduce both the vocabulary and also to focus on the sense of a document rather than deeper meaning,as there were so many stemming algorithms although a popular and long stading method is one of the algorithm know as Porter Stemming algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL_mhW4kIc-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "#split into words\n",
        "tokens = word_tokenize(text)\n",
        "#stemming of words\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYxy1-aAIc-S",
        "colab_type": "text"
      },
      "source": [
        "# 5.6 Additional Text Cleaning Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4c059igIc-S",
        "colab_type": "text"
      },
      "source": [
        "Handling large documents and large collections of text documents that do not fit intomemory.\n",
        "\n",
        "Extracting text from markup like HTML, PDF, or other structured document formats.\n",
        "\n",
        "Transliteration of characters from other languages into English.\n",
        "\n",
        "Decoding Unicode characters into a normalized form, such as UTF8.\n",
        "\n",
        "Handling of domain speci\fc words, phrases, and acronyms.\n",
        "\n",
        "Handling or removing numbers, such as dates and amounts.\n",
        "\n",
        "Locating and correcting common typos and misspellings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koSbPcOCIc-S",
        "colab_type": "text"
      },
      "source": [
        "# 5.7 Tips for Cleaning Text for Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2becgDLIc-T",
        "colab_type": "text"
      },
      "source": [
        "An embedding layer, for lack of a better name, is a word embedding that is learned jointly with a neural network model on a specific natural language processing task, such as language modeling or document classification. It requires that document text be cleaned and prepared such that each word is one-hot encoded.As the things always jump out at you when to take the time to review our data."
      ]
    }
  ]
}