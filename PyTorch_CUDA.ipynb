{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_CUDA.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORPw3dt-nYnX",
        "colab_type": "text"
      },
      "source": [
        "# CUDA \n",
        "\n",
        "To use a GPU, we need to allocate the tensor on the GPU's memory.\n",
        "\n",
        "The accessibilty to GPU is done by API called CUDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8QKNzzNhDEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "592f409c-5f30-423b-99f0-bc81dcc54adf"
      },
      "source": [
        "#importing Torch\n",
        "#checking for cuda\n",
        "\n",
        "import torch\n",
        "print (torch.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPyvAjy6hXCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd6c5e9-c451-4824-c5c0-1f557a625777"
      },
      "source": [
        "# preferred method: device agnostic tensor instantiation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVMJSxT_hkCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f947abb9-99e9-452d-c9a9-7912b6c7f6a3"
      },
      "source": [
        "def describe(y):\n",
        "    print(\"Type: {}\".format(y.type()))\n",
        "    print(\"Shape/size: {}\".format(y.shape))\n",
        "    print(\"Values: \\n{}\".format(y))\n",
        "\n",
        "    \n",
        "y = torch.rand(3, 3).to(device)\n",
        "describe(y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type: torch.cuda.FloatTensor\n",
            "Shape/size: torch.Size([3, 3])\n",
            "Values: \n",
            "tensor([[0.9240, 0.8138, 0.4332],\n",
            "        [0.3386, 0.8059, 0.0054],\n",
            "        [0.0177, 0.0477, 0.8087]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dyAsjizjWty",
        "colab_type": "text"
      },
      "source": [
        "# Unspoken rules of CUDA\n",
        "\n",
        "\n",
        "> Always have both the tensors in CUDA(Device-agonstic)\n",
        "\n",
        "*1. To operate CUDA and non-CUDA objects we need to make sure that both are on the same device(Multiple GPU's), if not we will break the code.*\n",
        "\n",
        "*2. The best practice to have while using multiple GPU's is to use CUDA_VISIBLE_DEVICES*\n",
        "\n",
        "*3. CUDA_VISIBLE_DEVICES = 0,1,2,3 python main.py.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Cross-GPU operations are not allowed by default, with the exception of copy_() and other methods with copy-like functionality such as to() and cuda(). Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cPxRu67iKqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "964b02af-09d5-4697-b37a-820a0dee7bb0"
      },
      "source": [
        "cuda = torch.device('cuda')\n",
        "s = torch.cuda.Stream()  # Create a new stream.\n",
        "y = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\n",
        "with torch.cuda.stream(s):\n",
        "    # sum() may start execution before normal_() finishes!\n",
        "    B = torch.sum(y)\n",
        "    print(B)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-83.5365, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRKi2QKl-4z",
        "colab_type": "text"
      },
      "source": [
        "**CUDA stream is linear sequence of execution**\n",
        "\n",
        "\n",
        "> The code is executed in the order of creation unless otherwise specified with either synchronize() or wait_stream(), default streams are responsible by PyTorch where as non-default as one above is user responsibility for syncronization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLnRocQleaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}