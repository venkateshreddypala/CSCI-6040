{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "How to Prepare Movie Review Data for Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxOvYjABJJdb",
        "colab_type": "text"
      },
      "source": [
        "# How to Prepare Movie Review Data for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiaNN6d9JJdd",
        "colab_type": "text"
      },
      "source": [
        "The Common task in NLP for a data scientist is sentimental Analysis,as it was a straight forward guide for creating a telugu movie review classifier in python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdjdXdkIJJde",
        "colab_type": "text"
      },
      "source": [
        "# 9.1 Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Luh4UGJJde",
        "colab_type": "text"
      },
      "source": [
        "A Bunch Collection of movie reviwes that are retrived from different sites review sites available on internet.As some of the reviews from a popular site had been collected and made available for the research on Natural Language Processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYHpaV-TJJdf",
        "colab_type": "text"
      },
      "source": [
        "# 9.2 Load Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF8CcC06JJdg",
        "colab_type": "text"
      },
      "source": [
        "Loading the individual text files for processing the directories of files assuming the review data that is currently available in the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsQIauZyJJdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load one file\n",
        "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noULL963JJdj",
        "colab_type": "text"
      },
      "source": [
        "It loads the document as ASCII and preserves any white space, like new lines. We can turn this into a function called load doc() that takes a filename of the document to load andreturns the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTYEn515JJdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y8f1IFBJJdl",
        "colab_type": "text"
      },
      "source": [
        "Having two directories each with having a limit of documents each,we can easily process the directory in turn by first getting a list of files in the directory using listdir() function leading a paticular document in the negative directory using the load_doc() to do the actual loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YydiqXLJJdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#specify directory to load\n",
        "directory = 'txt_sentoken/neg'\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "    #skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#load document\n",
        "doc = load_doc(path)\n",
        "print('Loaded %s' % filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od6H4aSDJJdo",
        "colab_type": "text"
      },
      "source": [
        "As we turn the processing the documents into the given function as wellas we use use it as template for later developing the function to clean the documents into a folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYj6KA7WJJdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#load all docs in a directory\n",
        "def process_docs(directory):\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "#skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#load document\n",
        "doc = load_doc(path)\n",
        "print('Loaded %s' % filename)\n",
        "#specify directory to load\n",
        "directory = 'txt_sentoken/neg'\n",
        "process_docs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a3iPRHgJJdq",
        "colab_type": "text"
      },
      "source": [
        "# 9.3 Clean Text Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMeVBX9gJJdr",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the data that we might want to do for a movie review data as we will use the bag-of-words model or word embedding which does not require the too much preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbHvt9SPJJdr",
        "colab_type": "text"
      },
      "source": [
        "# 9.3.1 Split into Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL-SMDc4JJds",
        "colab_type": "text"
      },
      "source": [
        "The raw tokens split by the white space as we are using the load_doc() function developed so we can split the function by split() the loaded document into the tokens that has been seperated by the white space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojQWSl1yJJdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#load the document\n",
        "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
        "text = load_doc(filename)\n",
        "#split into tokens by white space\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cQ9gOuYJJdv",
        "colab_type": "text"
      },
      "source": [
        "For cleaning a review there is an updated version cleaning the review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P88_YONaJJdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#load the document\n",
        "filename = 'txt_sentoken/neg/cv000_29416.txt'\n",
        "text = load_doc(filename)\n",
        "#split into tokens by white space\n",
        "tokens = text.split()\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('%s' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "tokens = re_punc.sub('', w) for w in tokens\n",
        "#remove remaining tokens that are not alphabetic\n",
        "tokens = word for word in tokens if word.isalpha()\n",
        "#filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = w for w in tokens if not w in stop_words\n",
        "#filter out short tokens\n",
        "tokens = word for word in tokens if len(word) > 1\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KFsITCjJJdy",
        "colab_type": "text"
      },
      "source": [
        "Calling a function clean_doc() this time for a positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL4y_f1XJJdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "#split into tokens by white space\n",
        "tokens = doc.split()\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('%s' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "tokens = re_punc.sub('', w) for w in tokens\n",
        "#remove remaining tokens that are not alphabetic\n",
        "tokens = word for word in tokens if word.isalpha()\n",
        "#filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = w for w in tokens if not w in stop_words\n",
        "#filter out short tokens\n",
        "tokens = word for word in tokens if len(word) > 1\n",
        "return tokens\n",
        "#load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGSyuLioJJd0",
        "colab_type": "text"
      },
      "source": [
        "# 9.4 Develop Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lwOIzyEJJd0",
        "colab_type": "text"
      },
      "source": [
        "While working with the predective models of the text such as bag-of-words model there will be a pressure for reducing the size of paticular vocabulary,as the larger vocabulary there is more for the sparse representation for each word in the document.For preparing the text for setimental analysis that involves defining as well as tailoring the vocabulary of words for the supported document by the model.\n",
        "\n",
        "As we can track the vocabulary in a paticular counter manner which is a dictionary of words and thier count with some additional convenience functions.As the function needs to call the previous developed load_doc() function as it is needed to clean the loaded document using the previous developed clean_doc() function and the last step is by calling the update() function on the counter object as the below function called as add_doc_to_vocab() to take the arguments for a document filename and for the counter vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u__qn5yAJJd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "#load doc\n",
        "doc = load_doc(filename)\n",
        "#clean doc\n",
        "tokens = clean_doc(doc)\n",
        "#update counts\n",
        "vocab.update(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF4Y1zYcJJd2",
        "colab_type": "text"
      },
      "source": [
        "Using the template for processing the documents in a directory of process_docs() and for updating it to call add_doc_to_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmXZsz9mJJd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "#skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4_X7waIJJd5",
        "colab_type": "text"
      },
      "source": [
        "Putting all of this together to develop a full vocabulary for all the documents in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRlF5SPQJJd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "#split into tokens by white space\n",
        "tokens = doc.split()\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('%s' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "tokens = re_punc.sub('', w) for w in tokens\n",
        "#remove remaining tokens that are not alphabetic\n",
        "tokens = word for word in tokens if word.isalpha()\n",
        "#filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = w for w in tokens if not w in stop_words\n",
        "#filter out short tokens\n",
        "tokens = word for word in tokens if len(word) > 1\n",
        "return tokens\n",
        "#load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "#load doc\n",
        "doc = load_doc(filename)\n",
        "#clean doc\n",
        "tokens = clean_doc(doc)\n",
        "#update counts\n",
        "vocab.update(tokens)\n",
        "#load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "#skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)\n",
        "#define vocab\n",
        "vocab = Counter()\n",
        "#add all docs to vocab\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "#print the size of the vocab\n",
        "print(len(vocab))\n",
        "#print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5AvzKAYJJd7",
        "colab_type": "text"
      },
      "source": [
        "The least common words that appear once across all revies that are not predictive as some of the words are used i common for all kinds of reviews the words generally appear once or a few times across it's lomit of 2,000 reviews are probably not predective and those can be removed from the vocabulary cutting down the tokens needed to be modeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WUE6e2qJJd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#keep tokens with > 5 occurrence\n",
        "min_occurane = 5\n",
        "tokens = k for k,c in vocab.items() if c >= min_occurane\n",
        "print(len(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Qi1l5TJJd9",
        "colab_type": "text"
      },
      "source": [
        "Defining a function called save_list() to save a list of items for such cases the tokens to files are one per line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAPQbtT-JJd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_list(lines, filename):\n",
        "data = '\\n'.join(lines)\n",
        "file = open(filename, 'w')\n",
        "file.write(data)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDKHzO0tJJeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "#split into tokens by white space\n",
        "tokens = doc.split()\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('%s' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "tokens = re_punc.sub('', w) for w in tokens\n",
        "#remove remaining tokens that are not alphabetic\n",
        "tokens = word for word in tokens if word.isalpha()\n",
        "#filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = w for w in tokens if not w in stop_words\n",
        "#filter out short tokens\n",
        "tokens = word for word in tokens if len(word) > 1\n",
        "return tokens\n",
        "#load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "#load doc\n",
        "doc = load_doc(filename)\n",
        "#clean doc\n",
        "tokens = clean_doc(doc)\n",
        "#update counts\n",
        "vocab.update(tokens)\n",
        "#load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "#skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)\n",
        "#save list to file\n",
        "def save_list(lines, filename):\n",
        "data = '\\n'.join(lines)\n",
        "file = open(filename, 'w')\n",
        "file.write(data)\n",
        "file.close()\n",
        "#define vocab\n",
        "vocab = Counter()\n",
        "#add all docs to vocab\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "#print the size of the vocab\n",
        "print(len(vocab))\n",
        "#print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "#keep tokens with > 5 occurrence\n",
        "min_occurane = 5\n",
        "tokens = k for k,c in vocab.items() if c >= min_occurane\n",
        "print(len(tokens))\n",
        "#save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwzGs1mSJJeB",
        "colab_type": "text"
      },
      "source": [
        "# 9.5 Save Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TSSdP5dJJeC",
        "colab_type": "text"
      },
      "source": [
        "Usig the cleaned data and choosing the vocabulary to prepare a movie review and save the already prepared versions of the paticular reviews for modeling as it was not a good practice as it decouples the data preparation from modeling and allowing to focus on modeling and circle back to data prep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmWGTYZsJJeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsuKbFHAJJeK",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the reviews and using the loaded vocab to filter out the unwanted tokens to save the clean reviews in a new file the approach could be saved all the positive type reviews in one file and all the negative type reviews in another file with a filtered tokens which are seperated by tge white space for the each review on separate lines.\n",
        "\n",
        "Defining a function for processing a document for cleaning it, filtering it, and also to return it as a single line which could be saved in a file,defining a function doc_to_line() for taking a filename and vocbulary as aruguments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0bVuzmUJJeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "#load the doc\n",
        "doc = load_doc(filename)\n",
        "#clean doc\n",
        "tokens = clean_doc(doc)\n",
        "#filter by vocab\n",
        "tokens = w for w in tokens if w in vocab\n",
        "return ' '.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBOLix0HJJeN",
        "colab_type": "text"
      },
      "source": [
        "A new version for defining the process_docs(),a list of lines are getting returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zesLNaRsJJeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "lines = list()\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "#skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "#add to list\n",
        "lines.append(line)\n",
        "return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZ7wnRNJJeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "#open the file as read only\n",
        "file = open(filename, 'r')\n",
        "#read all text\n",
        "text = file.read()\n",
        "#close the file\n",
        "file.close()\n",
        "return text\n",
        "#turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "#split into tokens by white space\n",
        "tokens = doc.split()\n",
        "#prepare regex for char filtering\n",
        "re_punc = re.compile('%s' % re.escape(string.punctuation))\n",
        "#remove punctuation from each word\n",
        "tokens = re_punc.sub('', w) for w in tokens\n",
        "#remove remaining tokens that are not alphabetic\n",
        "tokens = word for word in tokens if word.isalpha()\n",
        "#filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = w for w in tokens if not w in stop_words\n",
        "#filter out short tokens\n",
        "tokens = word for word in tokens if len(word) > 1\n",
        "return tokens\n",
        "#save list to file\n",
        "def save_list(lines, filename):\n",
        "data = '\\n'.join(lines)\n",
        "file = open(filename, 'w')\n",
        "file.write(data)\n",
        "file.close()\n",
        "#load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "#load the doc\n",
        "doc = load_doc(filename)\n",
        "#clean doc\n",
        "tokens = clean_doc(doc)\n",
        "#filter by vocab\n",
        "tokens = w for w in tokens if w in vocab\n",
        "return ' '.join(tokens)\n",
        "#load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "lines = list()\n",
        "#walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "    #skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "#create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "#load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "#add to list\n",
        "lines.append(line)\n",
        "return lines\n",
        "#load vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "#prepare negative reviews\n",
        "negative_lines = process_docs('txt_sentoken/neg', vocab)\n",
        "save_list(negative_lines, 'negative.txt')\n",
        "#prepare positive reviews\n",
        "positive_lines = process_docs('txt_sentoken/pos', vocab)\n",
        "save_list(positive_lines, 'positive.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}