{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supervised_training.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-WVClQ5zNww",
        "colab_type": "text"
      },
      "source": [
        "# Supervised Learning\n",
        "It is a way to observe a pattern for data that is already labelled and perform the classification.\n",
        "\n",
        "Supervised learning has the following requirements\n",
        "\n",
        "1.   Model\n",
        "2.   Loss Function\n",
        "3.   Training data\n",
        "4.   Optimization Algorithm\n",
        "\n",
        "\n",
        "1. Model, it can be  as follows [Preceptron](https://drive.google.com/open?id=1NXVv2q5o8NjCGl2ystJK5w03wsYEGFKX)\n",
        "\n",
        "We fine tune the probability to discrete classes using an evaluation dataset for a desired precision. After this step we need to choose a **[Loss Function](https://drive.google.com/open?id=1dA88ZYB-CGX-7tAg0nvxN8YZWB6UFfoq)** and an **Optmizer**\n",
        "\n",
        "The main function of an Optimizer is to observe the error signal and update the weight accordingly.\n",
        "\n",
        "A hyperparameter controls the decision made by the optimizer, the hypervisior is known as the **Learning Rate**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> PyTorch has several flavors of optimizers, such as Stochastic gradient descent(SGD), Adagard and Adam.\n",
        "\n",
        "> Start with the default learning rate and adjust accordingly.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz7ordtx2s4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation of Adam optimizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# since the binary classificatio\n",
        "input_dim = 2\n",
        "# defining learning rate\n",
        "lr = 0.001\n",
        "\n",
        "precep = Preceptron(input_dim = input_dim)\n",
        "bce_loss = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=preceptron.parameters(),lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRpfiOwP3bhP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> The training dataset is divided into batches and a hyperparameter named batch_size dfines the size of the training batch.\n",
        "\n",
        ">  An **epoch** is a complete iteration of the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQKyXfnFhz1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implemetation of a supervised training loop for  a preceptron and a binary classification\n",
        "\n",
        "for epoch_i in range(n_epochs):\n",
        "  # the inner loop is over the batches in the dataset\n",
        "  for batch_i in range(n_batches):\n",
        "\n",
        "    \n",
        "    #First Step is to get the data\n",
        "    x_data, y_target = get_toy_data(batch_size)\n",
        "    \n",
        "    # the next step is to clear the gradients\n",
        "    precep.zero_grad()\n",
        "    \n",
        "    # compute the forward pass of the mdel\n",
        "    y_pred = precep(x_data, apply_sigmoid = True)\n",
        "    \n",
        "    # Compute the loss value that we target to optimize\n",
        "    loss = bce_loss(y_pred, y_target)\n",
        "    \n",
        "    # propagate the loss signal backward\n",
        "    loss.backward()\n",
        "    \n",
        "    # Trigger the optimizer to perform one update\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NJ6pKsqkOcR",
        "colab_type": "text"
      },
      "source": [
        "> One needs to know the evaluation metrics according to the scope and definition of the project one thumbrule for splitting the data is 80 percent into training and 20 percent into testing but this is not true in all cases, in some examples we see validation set. \n",
        "\n",
        ">  We also need to stop the training  when the rate of growth is insignificant from epoch to epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFJNlq5p3U8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}