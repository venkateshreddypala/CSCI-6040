{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supervised_training.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-WVClQ5zNww",
        "colab_type": "text"
      },
      "source": [
        "# Supervised Learning\n",
        "It is a way to observe a pattern for data that is already labelled and perform the classification.\n",
        "\n",
        "Supervised learning has the following requirements\n",
        "\n",
        "1.   Model\n",
        "2.   Loss Function\n",
        "3.   Training data\n",
        "4.   Optimization Algorithm\n",
        "\n",
        "\n",
        "1. Model, it can be  as follows [Preceptron](https://drive.google.com/open?id=1NXVv2q5o8NjCGl2ystJK5w03wsYEGFKX)\n",
        "\n",
        "We fine tune the probability to discrete classes using an evaluation dataset for a desired precision. After this step we need to choose a **[Loss Function](https://drive.google.com/open?id=1dA88ZYB-CGX-7tAg0nvxN8YZWB6UFfoq)** and an **Optmizer**\n",
        "\n",
        "The main function of an Optimizer is to observe the error signal and update the weight accordingly.\n",
        "\n",
        "A hyperparameter controls the decision made by the optimizer, the hypervisior is known as the **Learning Rate**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> PyTorch has several flavors of optimizers, such as Stochastic gradient descent(SGD), Adagard and Adam.\n",
        "\n",
        "> Start with the default learning rate and adjust accordingly.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz7ordtx2s4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation of Adam optimizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# since the binary classificatio\n",
        "input_dim = 2\n",
        "# defining learning rate\n",
        "lr = 0.001\n",
        "\n",
        "precep = Preceptron(input_dim = input_dim)\n",
        "bce_loss = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=preceptron.parameters(),lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRpfiOwP3bhP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> The training dataset is divided into batches and a hyperparameter named batch_size dfines the size of the training batch.\n",
        "\n",
        ">  An **epoch** is a complete iteration of the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQKyXfnFhz1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implemetation of "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFJNlq5p3U8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}