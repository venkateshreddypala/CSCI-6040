{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "back_prop .ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2fdvef1iIb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNd4NCZrjiwi",
        "colab_type": "text"
      },
      "source": [
        "# Back Propagation (a distributed gradient descent technique) and Computation Graphs.\n",
        "Notions:\n",
        "1. xi - is an input to the neural network\n",
        "2. s is the output of the neural network.\n",
        "3. The j-th neuron of layer k receives the scalar input z(k) and produces the scalar activation output a(k).\n",
        "4. For the input layer, xj = z (1) j = a(1)\n",
        "5. W(k) is the transfer matrix that maps the output from the k-th layer to the input to the (k + 1)-th. Thus, W(1) = W and W(2) = UT.\n",
        "\n",
        "\n",
        "## Matrix Gradients\n",
        "## Computation Gradients\n",
        "\n",
        "### Pre-Requsite Knwoledge\n",
        "#### a. Regularization to prevent overfitting\n",
        "#### b. Vectorization\n",
        "#### c. Non-Linearity\n",
        "#### d. Initilization\n",
        "#### e. Optimizers\n",
        "#### f. Learning Rate\n",
        " "
      ]
    }
  ]
}