{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Project  Develop a Neural Bag of Words Model for Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrAVvAZSK3Bl",
        "colab_type": "text"
      },
      "source": [
        "# Project: Develop a Neural Bag-of-Words Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjQHO7G0K3Bm",
        "colab_type": "text"
      },
      "source": [
        "The reviews can be classified in both ways either it may be in a positive or it may be a negative review the evalutaion of the movie review is a classification for the sentimental analysis for developing a technique in regards to sentiment analysis models is to use a bag-of-words model to transform the selected document into vectors for each word in that selected document is assigned a score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdUHsbXMK3Bo",
        "colab_type": "text"
      },
      "source": [
        "# 10.1 Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N1NC7jMK3Bp",
        "colab_type": "text"
      },
      "source": [
        "Using the Movie Review dataset which was designed for sentimental analysis\n",
        "\n",
        "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
        "\n",
        "Using this link for a reviews dataset downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF3PGj01K3Bp",
        "colab_type": "text"
      },
      "source": [
        "# 10.2 Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83FzYaP1K3Bq",
        "colab_type": "text"
      },
      "source": [
        "For Preparing for the movie reviews dataset was already described \n",
        "\n",
        "1. Separation of Data into training and test sets\n",
        "\n",
        "2. Loading and Cleaning the data to remove punctuation and numbers\n",
        "\n",
        "3. Defining a vocabulary of preferred words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaX77Us8K3Br",
        "colab_type": "text"
      },
      "source": [
        "# 10.2.1 Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnxLnP3zK3Br",
        "colab_type": "text"
      },
      "source": [
        "Pretending and Developing the system which can predict the sentiment of a textual movie review either it may be in positive or it may be in negative which means after developing the model we can make predictions on the new textual reviews  requires all the same data preparation to get performed on these type of new reviews that are going to be performed on the traning data for this kind of model\n",
        "\n",
        "Spliting can be imposed easily by using the filenames of the reviews maned 000 to 899 are for traning data and reviews named 900 onwards to test the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1AgOx38K3Bs",
        "colab_type": "text"
      },
      "source": [
        "# 10.2.2 Loading and Cleaning Reviews "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GqL2WKdK3Bs",
        "colab_type": "text"
      },
      "source": [
        "As we know that text data is already clean so there is no requirement for the prepering it, Preparing the data like\n",
        "\n",
        "Spilt token on white space\n",
        "\n",
        "Remove all punctuation from words\n",
        "\n",
        "Remove all words that are not purely comprised of alphabetical characters\n",
        "\n",
        "Remove all words that are known stop words\n",
        "\n",
        "Remove all words that have a length less than or equal to 1 character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcylR3D2K3Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcLfJ5Y_K3Bw",
        "colab_type": "text"
      },
      "source": [
        "# 10.2.3 Define a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXWxRp-NK3Bx",
        "colab_type": "text"
      },
      "source": [
        "Define a Vocabulary is most important of know words which are usnga bag-of-words model,as of more words which are having the larger representation for the documents it is more important to constrain the words to only those belived to be predictive as it was difficult to know it beforehand and often it is more important to test the different hypotheses abotu how to construct a useful vovabulary.\n",
        "\n",
        "As we can develop a vocabulary as a counter which is a dictionary for mapping the words and thier cout that allow us to easily update and query as the each document can be added to this counter a we can easily seperate the negative directory that follows the positive directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZWBJh0nK3By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# update counts\n",
        "vocab.update(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k3tCKaOK3Bz",
        "colab_type": "text"
      },
      "source": [
        "Stepping into the vocabulary to remove all words that having a low occrrence are being used once or twice in all the avialble reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtHrU1xTK3B0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHDGVcSxK3B2",
        "colab_type": "text"
      },
      "source": [
        "Defining a new function called save_list() which saves the vocabulary for the required file with one word per file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0T5cr0LK3B2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "# convert lines to a single blob of text\n",
        "data = '\\n'.join(lines)\n",
        "# open file\n",
        "file = open(filename, 'w')\n",
        "# write text\n",
        "file.write(data)\n",
        "# close file\n",
        "file.close()\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eVWox-UK3B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# update counts\n",
        "vocab.update(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "# convert lines to a single blob of text\n",
        "data = '\\n'.join(lines)\n",
        "# open file\n",
        "file = open(filename, 'w')\n",
        "# write text\n",
        "file.write(data)\n",
        "# close file\n",
        "file.close()\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMcJKyhFK3B8",
        "colab_type": "text"
      },
      "source": [
        "# 10.3 Bag-of-Words Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcEg98JRK3B9",
        "colab_type": "text"
      },
      "source": [
        "Bag-of-Words model is a way of extracting features from the text so that the input of the text will be used with the machine learning algorithms like neural networks,each document is converted into a vector representation.The number of items in the vector representing a document corresponds to the number of words in the vocabulary.The words in any document that scores and that scores are placed in a corresponding location,converting the reviews into vectors which are ready for training a first neural network model\n",
        "\n",
        "1. Converting reviews to line of tokens\n",
        "\n",
        "2. Encoding reviews with a bag-of-words model representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npX3Oj2iK3B9",
        "colab_type": "text"
      },
      "source": [
        "# 10.3.1 Reviews to Lines of Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPpqWQi6K3B-",
        "colab_type": "text"
      },
      "source": [
        "As we can convert reviews to vectors for modeling firstly we have to clean them up as this involves in loading and performing the cleaning operation by filtering out words not in the chosen vocabulary byy convertig the remaining tokens in a single string or a line for encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k12mNg6K3B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "return ' '.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIAhhc2oK3CB",
        "colab_type": "text"
      },
      "source": [
        "After that we need a function to work through all the documents that are in the directory by converting the documents into the lines,the lists the process_docs() function that will not just expect a directory name and a vocabulary set as input arguments and returning a list of processed documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA_Uf69LK3CB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_9OQBNZK3CD",
        "colab_type": "text"
      },
      "source": [
        "The process_docs() consistently for both positive and negative reviews for constructing a dataset for review text and their associated output labels like 0 for negative and 1 for positive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTU3B6T9K3CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab)\n",
        "pos = process_docs('txt_sentoken/pos', vocab)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6GakrpwK3CG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab)\n",
        "pos = process_docs('txt_sentoken/pos', vocab)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "docs, labels = load_clean_dataset(vocab)\n",
        "# summarize what we have\n",
        "print(len(docs), len(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAaI4xYhK3CI",
        "colab_type": "text"
      },
      "source": [
        "# 10.3.2 Movie Reviews to Bag-of-Words Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXtVNhVyK3CJ",
        "colab_type": "text"
      },
      "source": [
        "Using the keras API to convert the reviews for encoding document vectors as this keras provides the tokenizer class that can do some of the cleaning and vocab definition tasks which will taken care,tokenizer class is one of the convenient which will easily transform the documents into the encoded vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg74sLYpK3CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TC-hkcpK3CL",
        "colab_type": "text"
      },
      "source": [
        "Documents that can be encoded using the Tokenizer by calling texts_to_matrix() this function takes documents list of both to encode and for an encoding mode this method is used to score the words in the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFxwq_eK3CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrJam2x-K3CO",
        "colab_type": "text"
      },
      "source": [
        "Adding an is_train argument and using to decide what kind of review files names will have to skip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd7g_Dl4K3CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHLY49mUK3CR",
        "colab_type": "text"
      },
      "source": [
        "Updating the dataset load_clean_dataset() for the entire train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrObeipUK3CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKPY12uZK3CV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRS3YvimK3CY",
        "colab_type": "text"
      },
      "source": [
        "# 10.4 Sentiment Analysis Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS-IBPnCK3CZ",
        "colab_type": "text"
      },
      "source": [
        "Developing a Multilayer Perceptron models to classify the encoded documents either it may be postive or negative as these models are very simple feed forward network models which are fully connected layers called as Dense in the keras deep learning library.\n",
        "\n",
        "1. First sentimental analysis model\n",
        "\n",
        "2. Comparing word scoring models\n",
        "\n",
        "3. Making a prediciton for new reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwR8Pge7K3Cc",
        "colab_type": "text"
      },
      "source": [
        "# 10.4.1 First Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtlYePpjK3Cd",
        "colab_type": "text"
      },
      "source": [
        "Developing a simple MLP model for predecting the sentiment for the encoded type of reviews,will having the input layer of this model that equals to the number of words in the vocabulary and turn the length of the input documents so that we can store a new variable called n_words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B3lnsRyK3Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_words = Xtest.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmfg7TPpK3Cf",
        "colab_type": "text"
      },
      "source": [
        "For defining a network all the models configuration that was found with very a very little trail and error and those are not considered for this kind of problem.Using a single hidden layer with 50 neurons that is rectified linear activation function the output layer is a single neuron having a sigmoid activation function for prediciting 0 for negative and 1 for positive reviews.Keeping a track of accuracy while traning and evaluating this kind of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa59Py_4K3Cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(n_words):\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "model.summary()\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKw7vxNFK3Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "model.summary()\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "return model\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "# define the model\n",
        "n_words = Xtest.shape[1]\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jufrdeWeK3Cl",
        "colab_type": "text"
      },
      "source": [
        "# 10.5 Comparing Word Scoring Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7bNOETJK3Cm",
        "colab_type": "text"
      },
      "source": [
        "text_to_matrix() is a function for the Tokenizer in the keras API provides 4 differnet methods those are Binary, Count, TFIDF, Frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht_QPwTvK3Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare bag-of-words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "# encode training data set\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "# encode training data set\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "return Xtrain, Xtest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX9nyp-uK3Cq",
        "colab_type": "text"
      },
      "source": [
        "Evalutaing the MLP for a given specific ecoding for the data  the neural networks are so stochastic that they are producing the different results when the same model is going to fit on the same data this mainly because for the random initial weights that are shuffling for the patterns during the mini-batch gradient descent,evaluating the MLP by traning it on the train set while estimating the skill on the test set for 10 times and returns a list of accuracy scores across all these runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS9G75eaK3Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "scores = list()\n",
        "n_repeats = 30\n",
        "n_words = Xtest.shape[1]\n",
        "for i in range(n_repeats):\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "scores.append(acc)\n",
        "print('%d accuracy: %s' % ((i+1), acc))\n",
        "return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP6UAzntK3Ct",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the performance for the 4 different word scoring methods by pulling all this together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlN4kzoNK3Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "    continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "return model\n",
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "scores = list()\n",
        "n_repeats = 10\n",
        "n_words = Xtest.shape[1]\n",
        "for i in range(n_repeats):\n",
        "# define network\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "# evaluate\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "scores.append(acc)\n",
        "print('%d accuracy: %s' % ((i+1), acc))\n",
        "return scores\n",
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "# encode training data set\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "# encode training data set\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "return Xtrain, Xtest\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "# prepare data for mode\n",
        "Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "# evaluate model on data for mode\n",
        "results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
        "# summarize results\n",
        "print(results.describe())\n",
        "# plot results\n",
        "results.boxplot()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiY4ab4vK3Cw",
        "colab_type": "text"
      },
      "source": [
        "Making predections for a new review texts having a healthy example with the both a clearly positive and a clearly negative review using the simpleMPL which was developed above with the frequency word scoring mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klL6d568K3Cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH-6MSZRK3C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "# load the doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab)\n",
        "pos = process_docs('txt_sentoken/pos', vocab)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "# define network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "model.summary()\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "return model\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "# clean\n",
        "tokens = clean_doc(review)\n",
        "# filter by vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "# convert to line\n",
        "line = ' '.join(tokens)\n",
        "# encode\n",
        "encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "# predict sentiment\n",
        "yhat = model.predict(encoded, verbose=0)\n",
        "# retrieve predicted percentage and label\n",
        "percent_pos = yhat[0,0]\n",
        "if round(percent_pos) == 0:\n",
        "return (1-percent_pos), 'NEGATIVE'\n",
        "return percent_pos, 'POSITIVE'\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab)\n",
        "test_docs, ytest = load_clean_dataset(vocab)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "# define network\n",
        "n_words = Xtrain.shape[1]\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}