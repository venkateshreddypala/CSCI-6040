{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "corpus.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMk84uWTrQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFNLs0aOTstq",
        "colab_type": "text"
      },
      "source": [
        "# Build a Corpus\n",
        "We are just playing with data that is already available, but in real world there's no data that is ready for you to extract the features.\n",
        "\n",
        "This noteboook demonstrates how to gather data on your own based on your requirements.\n",
        "\n",
        "We use  [beautifulsoup4](https://pypi.org/project/beautifulsoup4/) , [newspapaer3k](https://pypi.org/project/newspaper3k/0.2.2/)for this project, you can install the same using the below command\n",
        "\n",
        "\n",
        "```\n",
        "pip install beautifulsoup4\n",
        "\n",
        "pip install newspaper3k\n",
        "```\n",
        "\n",
        "Scraping is not encouraged as it creates unncessary burden on servers and you might be prohibited from visiting the website again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-C67mXVY2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing required libraries\n",
        "!pip install beautifulsoup4\n",
        "!pip install newspaper3k\n",
        "import os\n",
        "import uuid\n",
        "import atexit\n",
        "import urllib\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep, time\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article, ArticleException\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273DRVS8Vela",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the url you want to scrape\n",
        "\n",
        "POCKET_BASE_URL = 'https://getpocket.com/explore/%s'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-v7VtvW4CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting what is required from the url, you can pin point to headins\n",
        "# observe the pattern before performing scraping\n",
        "\n",
        "df = pd.DataFrame(columns=['title', 'excerpt', 'url', 'file_name', \"keyword\", \"category\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W8Vn7yoXEH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure to store the data in CSV file\n",
        "@atexit.register\n",
        "def save_dataframe():\n",
        "\n",
        "  dataframe_name = \"dataframe_{0}.csv\".format(time())\n",
        "  df.to_csv(dataframe_name, index=False)\n",
        "  # randomized crawling to prevent from getting banned\n",
        "categories = list(CATEGORIES.items())\n",
        "random.shuffle(categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcA_VALnXIds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for category_name, keywords in categories:\n",
        "  print(\"Exploring Category=\\\"{0}\\\"\".format(category_name))\n",
        "  for kw in keywords:\n",
        "# Get trending content from Pocket's explore endpoint\n",
        "    result = requests.get(POCKET_BASE_URL % urllib.parse.quote_plus(kw))\n",
        "# Extract the media items\n",
        "    soup = BeautifulSoup(result.content, \"html5lib\")\n",
        "    media_items = soup.find_all(attrs={'class': 'media_item'})\n",
        "    for item_html in media_items:\n",
        "        title_html = item_html.find_all(attrs={'class': 'title'})[0]\n",
        "        title = title_html.text\n",
        "        url = title_html.a['data-saveurl']\n",
        "        print(\"Indexing article: \\\"{0}\\\" from \\\"{1}\\\"\".format(title, url))\n",
        "        excerpt = item_html.find_all(attrs={'class': 'excerpt'})[0].text\n",
        "        try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        content = article.text\n",
        "      except ArticleException as e:\n",
        "        print(\"Encoutered exception when parsing \\\"{0}\\\": \\\"{1}\\\"\".format(url, str(e)))\n",
        "        continue\n",
        "      if not content:\n",
        "        print(\"Couldn't extract content from \\\"{0}\\\"\".format(url))\n",
        "        continue\n",
        "      # Save the text file\n",
        "      file_name = \"{0}.txt\".format(str(uuid.uuid4()))\n",
        "      with open('./data/files/{0}'.format(file_name), 'w+') as text_file:\n",
        "          text_file.write(content)\n",
        "      # Append the row in our dataframe\n",
        "      df.loc[len(df)] = [title, excerpt, url, file_name, kw, category_name]\n",
        "      # Need to sleep in order to not get blocked\n",
        "      sleep(random.randint(5, 15))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKANF0JYXdv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}