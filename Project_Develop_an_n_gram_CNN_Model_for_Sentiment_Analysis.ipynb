{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Project  Develop an n-gram CNN Model for Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVi0bXbLC6-",
        "colab_type": "text"
      },
      "source": [
        "# Project: Develop an n-gram CNN Model for Sentiment AnalysisProject: Develop an n-gram CNN Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcYCxhuBLC6_",
        "colab_type": "text"
      },
      "source": [
        "Standard deep learning model for the text classification and the sentiment analyis which uses a word embedding layer and also one dimensional convolutional neural network and the model which can be expanded by using the multiple parallel convolutional neural networks which reads the sources document using the different kernel sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kjh-p7bLC7A",
        "colab_type": "text"
      },
      "source": [
        "Putting all these steps into a function called clean_doc() which takes an argument that the raw text load froma file and returns a list of the cleaned tokens.Defining a function load_doc() and the file ready to use with the clean_doc() function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BexcjdLMLC7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95MG5cHiLC7D",
        "colab_type": "text"
      },
      "source": [
        "# Clean All Reviews and Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvOgrkADLC7D",
        "colab_type": "text"
      },
      "source": [
        "using the functions to clean the reviews and applying it to all the reviews for developing a new function named process_docs() will going walk through all the reviews in a directory and the full function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLbIjT37LC7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "documents = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load the doc\n",
        "doc = load_doc(path)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# add to list\n",
        "documents.append(tokens)\n",
        "return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiuFqNQcLC7G",
        "colab_type": "text"
      },
      "source": [
        "using python list comprehension for creating the labels for the negative(0) and the positive(2) for both the train and test sets and function below named as load_clean_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLx3P3rcLC7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', is_train)\n",
        "pos = process_docs('txt_sentoken/pos', is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjUl6MU2LC7J",
        "colab_type": "text"
      },
      "source": [
        "For saving the prepared train and test sets to file so that we can load them later for modeling and model evaluation the function below named save_dataset() saving the given prepared dataset to a file using the pickle API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MJEsWRkLC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save a dataset to file\n",
        "def save_dataset(dataset, filename):\n",
        "    dump(dataset, open(filename, 'wb'))\n",
        "print('Saved: %s' % filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvXEaZKfLC7M",
        "colab_type": "text"
      },
      "source": [
        "Putting all this together of the data preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6K40WGWLC7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "tokens = ' '.join(tokens)\n",
        "return tokens\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "documents = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load the doc\n",
        "doc = load_doc(path)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# add to list\n",
        "documents.append(tokens)\n",
        "return documents\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', is_train)\n",
        "pos = process_docs('txt_sentoken/pos', is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "return docs, labels\n",
        "# save a dataset to file\n",
        "def save_dataset(dataset, filename):\n",
        "dump(dataset, open(filename, 'wb'))\n",
        "print('Saved: %s' % filename)\n",
        "# load and clean all reviews\n",
        "train_docs, ytrain = load_clean_dataset(True)\n",
        "test_docs, ytest = load_clean_dataset(False)\n",
        "# save training datasets\n",
        "save_dataset([train_docs, ytrain], 'train.pkl')\n",
        "save_dataset([test_docs, ytest], 'test.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUEpE9SiLC7O",
        "colab_type": "text"
      },
      "source": [
        "# 16.3 Develop Multichannel Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg8Elj_SLC7P",
        "colab_type": "text"
      },
      "source": [
        "Developing a multichannel convolutional neural network for the sentiment analysis orediciton problem \n",
        "\n",
        "Encode Data\n",
        "\n",
        "Define Model\n",
        "\n",
        "Complete Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5mZK1aZLC7P",
        "colab_type": "text"
      },
      "source": [
        "# 16.3.1 Encode Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l20RgsXsLC7Q",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the training the dataset the function below-named load_dataset() called as load the pickled training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S-s9wdxLC7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "return load(open(filename, 'rb'))\n",
        "trainLines, trainLabels = load_dataset('train.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m9k98AbLC7U",
        "colab_type": "text"
      },
      "source": [
        "The function create_tokenizer() is going to create a tokenizer with a list of the documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5seQJBuLC7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WWfL4w8LC7Y",
        "colab_type": "text"
      },
      "source": [
        "Needing to know the maximum length for the input sequences as a input for a model and to pad all the sequences upto a fixed length and the function max_length()  will allow us to calculate the maximumm length to all the reviews in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XHkdIhlLC7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "return max([len(s.split()) for s in lines])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylfKyTxbLC7b",
        "colab_type": "text"
      },
      "source": [
        "To know the size of the vocabulary for the embedding layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tN_8jYeLC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTN6mgBELC7d",
        "colab_type": "text"
      },
      "source": [
        "Integer encoding the pad to clean the movie review text and below it was named as encode_text() having the both encode pad text data to the maximum review length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73IRzxK2LC7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "# integer encode\n",
        "encoded = tokenizer.texts_to_sequences(lines)\n",
        "# pad encoded sequences\n",
        "padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "return padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyAIDnldLC7f",
        "colab_type": "text"
      },
      "source": [
        "# 16.3.2 Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mubBUamvLC7g",
        "colab_type": "text"
      },
      "source": [
        "Standard model for the document classification were use for embedding layer as input which was followed by a one-dimensional convolution neural network.\n",
        "\n",
        "Input layer that was defining the length of the input sequences\n",
        "\n",
        "Embedding the layer set size of the vocabulary and the 100-dimensional real-valued representations\n",
        "\n",
        "Conv1D layer with 32 filters and a kernel size which will set to a number of words to read at once.\n",
        "\n",
        "MaxPooling1D layer for consolidating the output from the convolutional layer.\n",
        "\n",
        "Flatten layer was to reduce the three-dimensional output to a two dimensional concatenation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEgZvXN6LC7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "# channel 1\n",
        "inputs1 = Input(shape=(length,))\n",
        "embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "drop1 = Dropout(0.5)(conv1)\n",
        "pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "flat1 = Flatten()(pool1)\n",
        "# channel 2\n",
        "inputs2 = Input(shape=(length,))\n",
        "embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "drop2 = Dropout(0.5)(conv2)\n",
        "pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "flat2 = Flatten()(pool2)\n",
        "# channel 3\n",
        "inputs3 = Input(shape=(length,))\n",
        "embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "drop3 = Dropout(0.5)(conv3)\n",
        "pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "flat3 = Flatten()(pool3)\n",
        "# merge\n",
        "merged = concatenate([flat1, flat2, flat3])\n",
        "# interpretation\n",
        "dense1 = Dense(10, activation='relu')(merged)\n",
        "outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "# compile\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZvtmyKwLC7j",
        "colab_type": "text"
      },
      "source": [
        "Putting all this together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bHZsOglLC7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "return load(open(filename, 'rb'))\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "return max([len(s.split()) for s in lines])\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "# integer encode\n",
        "encoded = tokenizer.texts_to_sequences(lines)\n",
        "# pad encoded sequences\n",
        "padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "return padded\n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "# channel 1\n",
        "inputs1 = Input(shape=(length,))\n",
        "embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "drop1 = Dropout(0.5)(conv1)\n",
        "pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "flat1 = Flatten()(pool1)\n",
        "# channel 2\n",
        "inputs2 = Input(shape=(length,))\n",
        "embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "drop2 = Dropout(0.5)(conv2)\n",
        "pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "flat2 = Flatten()(pool2)\n",
        "# channel 3\n",
        "inputs3 = Input(shape=(length,))\n",
        "embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "drop3 = Dropout(0.5)(conv3)\n",
        "pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "flat3 = Flatten()(pool3)\n",
        "# merge\n",
        "merged = concatenate([flat1, flat2, flat3])\n",
        "# interpretation\n",
        "dense1 = Dense(10, activation='relu')(merged)\n",
        "outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "# compile\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True, to_file='model.png')\n",
        "return model\n",
        "# load training dataset\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "# define model\n",
        "model = define_model(length, vocab_size)\n",
        "# fit model\n",
        "model.fit([trainX,trainX,trainX], trainLabels, epochs=7, batch_size=16)\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja2rYKjeLC7p",
        "colab_type": "text"
      },
      "source": [
        "# 16.4 Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSOJDQflLC7q",
        "colab_type": "text"
      },
      "source": [
        "Using the loaded data functions that are developed we can actually load and encode the both training and the test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e364yl73LC7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Max document length: %d' % length)\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "print(trainX.shape, testX.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si3qZCR8LC7s",
        "colab_type": "text"
      },
      "source": [
        "Loading and evaluating the model on both the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMEiqM_oLC7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "return load(open(filename, 'rb'))\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "return max([len(s.split()) for s in lines])\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "# integer encode\n",
        "encoded = tokenizer.texts_to_sequences(lines)\n",
        "# pad encoded sequences\n",
        "padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "return padded\n",
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate([trainX,trainX,trainX], trainLabels, verbose=0)\n",
        "print('Train Accuracy: %.2f' % (acc*100))\n",
        "# evaluate model on test dataset dataset\n",
        "_, acc = model.evaluate([testX,testX,testX], testLabels, verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}