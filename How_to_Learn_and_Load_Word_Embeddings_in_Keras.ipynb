{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "How to Learn and Load Word Embeddings in Keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPNkkJv6I_XZ",
        "colab_type": "text"
      },
      "source": [
        "# How to Learn and Load Word Embeddings in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgiBoqT-I_Xb",
        "colab_type": "text"
      },
      "source": [
        "Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-GllsedI_Xc",
        "colab_type": "text"
      },
      "source": [
        "# 13.1 Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z30XpXnI_Xc",
        "colab_type": "text"
      },
      "source": [
        "Word embedding is a class act which approaches for representing the words and documents using a dense vector representation as this was an improvement over for more traditional bag-of-word models for encoding schemes where the large sparse vectors were used for represent the each word or the entire voabulary and the given word or the document was represented by a large vector comprised mostly of the zero values.\n",
        "\n",
        "And the position of a word within the vector space is learned from text and is based on the words that surround by another word when it was used where there are two methods for learning the word embedding from text\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_j4eesmI_Xd",
        "colab_type": "text"
      },
      "source": [
        "# 13.2 Keras Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjusHf6-I_Xe",
        "colab_type": "text"
      },
      "source": [
        "Keras offering the embedding layer which can be used for the neural network on the text data as it requires input data be integer encoded so that each word was represented by a unique integer as the preparation for the data can be performed using the Tokenizer API by providing with this keras.\n",
        "\n",
        "As the embedding layer was initialized with some random weights and those will learn an embedding for all the words of the traning dataset,and also it is flexible layer which was used in different ways so that \n",
        "\n",
        "It can be used alone for learning word embedding which can be saved and used for another model later.\n",
        "\n",
        "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "\n",
        "It can be used for loading a pre-trained word for embedding model as it was a type of transfer learning.\n",
        "\n",
        "As the embedding layer was defined by the first hidden layer of a network as it must specify 3 kinds\n",
        "\n",
        "input_dim: The size of the vocabulary in the given text data as the data is a integer encoded for the values between 0-10 the size of the vocabulary was about 11 words.\n",
        "\n",
        "output_dim: Size of the vector space in the words which are embedded as it defines the size of the output vectors from the layer for each words so that it could be around 32 to 100 or even more larger than that.\n",
        "\n",
        "input_length: Length of the input sequences that could define any input layer of the keras model as all the input documents are comprised for 1000 words and that would be 1000. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBftPMxXI_Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = Embedding(200, 32, input_length=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etafp57HI_Xh",
        "colab_type": "text"
      },
      "source": [
        "# 13.3 Example of Learning an Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Ujhn-AI_Xi",
        "colab_type": "text"
      },
      "source": [
        "Looking into how to learn the word embedding while fitting a neural network on a text classification problems which was defining a small problem where we have about 10 documents with a comment each on their piece of work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM4odxPEI_Xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!',\n",
        "'Weak',\n",
        "'Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "# define class labels\n",
        "labels = [1,1,1,1,1,0,0,0,0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqPAg0GwI_Xk",
        "colab_type": "text"
      },
      "source": [
        "As the integer encode each document that means having the input of the embedding layer will having the sewuences of integers so the experiment with the other sophisticated bag of word model encoded like counts or TF-IDF.keras provide one-hot encoding method which was simple methos for representing each word using a one-hot vector and this sparse representation is very inefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ0pt1GgI_Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcQzAEFoI_Xn",
        "colab_type": "text"
      },
      "source": [
        "Dedining the embedding layer as a part for our neural network model the embedding layer having the vocabulary of 50 and having the input length of 4 and also choosing it a small embedding space of 8 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56BubVfI_Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gPyom2UI_Xp",
        "colab_type": "text"
      },
      "source": [
        "Finally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmIPLi6bI_Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C0sJ3lNI_Xr",
        "colab_type": "text"
      },
      "source": [
        "Complete code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27-FPEU5I_Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "        'Great effort',\n",
        "'nice work',\n",
        "'Excellent!',\n",
        "'Weak',\n",
        "'Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "# define class labels\n",
        "labels = [1,1,1,1,1,0,0,0,0,0]\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-BynhxYI_Xv",
        "colab_type": "text"
      },
      "source": [
        "# 13.4 Example of Using Pre-Trained GloVe Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtTgJVRNI_Xv",
        "colab_type": "text"
      },
      "source": [
        "As the machine learning approaches targeting NLP that have been basing on the shallow models that are trained on a very high dimensional and sparse features.And the smallest package for embedding was about 822 Mega bytes called as glove.6B.zip and it was trained on a dataset for a billion tokens with the help of a vovabulary of having the 400 thousand words.\n",
        "\n",
        "As the keras providing the tokenizer class to be in fit in the training data that converts the text sequences which are consistently calling them texts_to_sequences() method on the tokenizer class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0FgWMFxI_Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!',\n",
        "'Weak',\n",
        "'Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "# define class labels\n",
        "labels = [1,1,1,1,1,0,0,0,0,0]\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ujbfKS5I_Xz",
        "colab_type": "text"
      },
      "source": [
        "after that we need to load the entire GloVe file into memory as a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i09088NI_Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "values = line.split()\n",
        "word = values[0]\n",
        "coefs = asarray(values[1:], dtype='float32')\n",
        "embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pObiV5-1I_X1",
        "colab_type": "text"
      },
      "source": [
        "As this was some what slow but it might be better if we filter the embedding of the unique words in data after that creating a matrix of one embedded each word in the dataset as a result a matrix of weights only for words we will seen during training session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fieAY3xEI_X2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "embedding_vector = embeddings_index.get(word)\n",
        "if embedding_vector is not None:\n",
        "embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTCE_PJgI_X7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN2A3SPRI_X8",
        "colab_type": "text"
      },
      "source": [
        "Complete worked "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtGr1YaI_X9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!',\n",
        "'Weak',\n",
        "'Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "# define class labels\n",
        "labels = [1,1,1,1,1,0,0,0,0,0]\n",
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt', mode='rt', encoding='utf-8')\n",
        "for line in f:\n",
        "values = line.split()\n",
        "word = values[0]\n",
        "coefs = asarray(values[1:], dtype='float32')\n",
        "embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "embedding_vector = embeddings_index.get(word)\n",
        "if embedding_vector is not None:\n",
        "embedding_matrix[i] = embedding_vector\n",
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA0BKHjxI_X_",
        "colab_type": "text"
      },
      "source": [
        "# 13.5 Tips for Cleaning Text for Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUc0tU-MI_X_",
        "colab_type": "text"
      },
      "source": [
        "In the field of NLP that has been moving away from the bag-of-word model and word encoding towards the word embeddings as of it the benefit for this word embeddings was to encode each word into a dense vector which captures about something within the text data.That means the variation of words which are like case spelling punctuation and so on that will be automatically learned to be similar in the embedding space."
      ]
    }
  ]
}