{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ann_intro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4lkztNU-n-Q",
        "colab_type": "text"
      },
      "source": [
        "# The Preceptron\n",
        "\n",
        "We refer a unit of Neural Network as a Preceptron.\n",
        "\n",
        "![A simple perceptron with an input (x) and an output (y). The weights (w) and bias (b)](https://drive.google.com/file/d/1T3_Cars4GCfKPLhMVRe0LTPU09wzPsLb/view?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "y = f ( w x + b )\n",
        " \n",
        " \n",
        " In practical usage there are more than one input, f is an activation function, wx+b is a linear function also known as affine transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkoDVJsxAtbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Constructing a class to build a single layer Preceptron\n",
        "class Percep(nn.Module):\n",
        "    # input_dim --> represents the size of the input features\n",
        "    def __init__(self, input_dim):\n",
        "       \n",
        "        super(Percep, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "    # defining a function for he forward pass of the preceptron   \n",
        "    def forward(self, x_in):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor \n",
        "                x_in.shape should be (batch, num_features)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,).\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAip3ddODZq1",
        "colab_type": "text"
      },
      "source": [
        "PyTorch efficeintly offers a Linear class in torch.nn module and also keep tracks of weights and biases.\n",
        "\n",
        "Sigmoid ( $f ( x ) = \\frac { 1 } { 1 + e ^ { - x } }$ ) is the most popular activation function. There's a disadvatage of using the sigmoid in early stage of NN as it results in vanisihing gradient or exploding gradients problem.\n",
        "\n",
        "To avoid the vanishing gradinet problem there's ReLU (f(x)=max(0,x)), this clips negative values to zero, but sometimes the network goes to zero and never revive again, referred as dying ReLU problem.\n",
        "There are variants to deal with this problem such as Leaky ReLU and Parametric ReLU, where the leak coefficient a is a learned parameter f(x)=max(x,ax).  \n",
        "\n",
        "There are other activation functions such as \n",
        "$\\operatorname { softmax } \\left( x _ { i } \\right) = \\frac { e ^ { x _ { i } } } { \\sum _ { j = 1 } ^ { k } e ^ { y } }$    mainly used for classification tasks and  Tanh  $f ( x ) = \\tanh x = \\frac { e ^ { x } - e ^ { - x } } { e ^ { x } + e ^ { - x } }$ which is a variant of sigmoid function\n",
        "\n"
      ]
    }
  ]
}