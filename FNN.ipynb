{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "126vm3Cc2AH2",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing for Neural Networks\n",
        "\n",
        "In the previous Notebooks we observed how to implement a preceptron, but preceptrons are incapable learning from any nontrivial patterns present in data, so for complex solutions in the area of NLP&NLU  **Feed Forward Neural Networks** are adapted.\n",
        "\n",
        "Feed Forward Neural Networks is a gneric term, to classify in this note book we are going to see about the two important types, one is a **Multi-layer preceptron(MLP)**  which is simply an extension for single layer precpetron.\n",
        "\n",
        "Other type of Neural Network that we would look into is a **Convolutional Neural Network(CNN).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfG1ys28-EV2",
        "colab_type": "text"
      },
      "source": [
        "**MLP's** are composed of three stages of representation and two Linear layers.\n",
        "\n",
        "The first stage is input vector, it is one-hot representation.\n",
        "Then passed to a Linear layer the vector computes the hidden vector, this layer is compressed between the input and the output layer. \n",
        "\n",
        "One advantage of PyTorch is it automatically figures out how to do the backward pass and gradient updates based on the definition of the model and also the implementation of the forward pass. In this examplewe only implement the forward pass of the backpropagation and leaves the rest of computation to PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E_JvLdCHQ-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementation of MLP\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultilayerPerceptron(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    \"\"\"\n",
        "    input_dim (int): the size of the input vectors\n",
        "    hidden_dim (int): the output size of the first Linear layer\n",
        "    output_dim (int): the output size of the second Linear layer\n",
        "    \"\"\"\n",
        "    \n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "  def forward(self, x_in, apply_softmax=False):\n",
        "    intermediate = F.relu(self.fcl(x_in))\n",
        "    output = self.fc2(intermediate)\n",
        "    \n",
        "    if apply_softmax:\n",
        "      output = F.softmax(output, dim = 1)\n",
        "    return output\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJx39s9vNWmG",
        "colab_type": "text"
      },
      "source": [
        "The function created takes the input of data in a tensor, we define the shape should be (batch, input_dim). apply sotmax activates the inbuilt sotmax function. This function returns an output  with specified shape and dimension.\n",
        "\n",
        "In the below, to illustrate, we choose a size 3 dimension for the input vector, an output dimension of size 4 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BSAIDlrNSkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e6b54649-2e77-4139-9e4a-ef8871297e6a"
      },
      "source": [
        "batch_size = 2 # number of samples input at once\n",
        "input_dim = 3\n",
        "hidden_dim = 100\n",
        "output_dim = 4\n",
        "\n",
        "# Initialize model\n",
        "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
        "print(mlp)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultilayerPerceptron(\n",
            "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_g9PMQBO5iJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}