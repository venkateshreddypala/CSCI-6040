{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Project.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAYSwCFxLUmd",
        "colab_type": "text"
      },
      "source": [
        "# Develop an Embedding + CNN Model for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuroEGTLUmf",
        "colab_type": "text"
      },
      "source": [
        "Having the similar meaning and a similar real-valued vector representation for the word embedding techinique to represent a text of different words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-5z7X1sLUmf",
        "colab_type": "text"
      },
      "source": [
        "# 15.1 Movie Review Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgL6uqXtLUmg",
        "colab_type": "text"
      },
      "source": [
        "The Dataset is designed for the sentimental analysis which was described http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz \n",
        "Movie Review Polarity Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4wBhz1QLUmg",
        "colab_type": "text"
      },
      "source": [
        "# 15.2 Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoZixLgnLUmh",
        "colab_type": "text"
      },
      "source": [
        "Preparation for the movie dataset was already described\n",
        "\n",
        "1. Seperation of data into training and test sets.\n",
        "\n",
        "2. Loading and Cleaning the data to remove ppuncutation and numbers\n",
        "\n",
        "3. Defining a Vocabulary of preferred words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mNUiDiHLUmi",
        "colab_type": "text"
      },
      "source": [
        "# 15.2.1 Split into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDKtxseLUmi",
        "colab_type": "text"
      },
      "source": [
        "Predecting what we have been developed a system which can predict the sentiment of the textual movie review either it may be positive or a negative which means after developing the model we need to make the predicitons for the new text reviews,requiring all the same data preparation that could be performed on the new type reviews which was been performed on the training data for that kind of model,ensuring the constraint which was built in the evalluation of our model was by splitting the training and the test datasets are prior to any other data preparations,the split can be imposed easily by using the filenames of the reviews where those are named 000 to 899 and also for the traning data and reviews which are named 900."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acEwdDXuLUmj",
        "colab_type": "text"
      },
      "source": [
        "# 15.2.2 Loading and Cleaning Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD7DyS6kLUmj",
        "colab_type": "text"
      },
      "source": [
        "As the text was already pretty clean and not much preparations was required without getting bogged down too much in details we are going to prepare the data using \n",
        "\n",
        "Split tokens on white space\n",
        "\n",
        "Removing all the puncutations from words\n",
        "\n",
        "Removing all words that are not purely comprised of alphabetical characters\n",
        "\n",
        "Removing all the words that are known to stop the words\n",
        "\n",
        "Removing all the words that are having a length less than or equal to 1 character\n",
        "\n",
        "putting all of this steps together tooking step into a function called clean_doc() which takes the argument of the raw text loaded from a file and that returns a list of the cleaned tokens defining a function load_doc() which loads a document when a file was ready for using the clean_doc() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBCWVyGhLUmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load the document\n",
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-Y3gSGMLUmm",
        "colab_type": "text"
      },
      "source": [
        "# 15.2.3 Define a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgUEKxOKLUmn",
        "colab_type": "text"
      },
      "source": [
        "As it was one of the important for defining a vocabulary of the known words for using a text model the more words the larger representations of the documents as it was important to constrain the words only those who are belived to be more predictive as it was difficult to know beforehand and often as it was important to test the different hypotheses about how to construct a useful vocabulary as we already seen how to remove the punctuation and also how to build a set of all the known words,developing a vocabulary as a counter the dictionary mapping of the words and the count which allows us to update the query as the each document which can be added to a counter that can step all over the reviews in the both ways either it may be the negative directory or the positive directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByOH2qCmLUmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# update counts\n",
        "vocab.update(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iguMDGuxLUmp",
        "colab_type": "text"
      },
      "source": [
        "stepping through the vocabulary to remove all the words that was having a low occurences such as only being used once or twice in all the reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5npc6UtsLUmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66EngM-aLUmr",
        "colab_type": "text"
      },
      "source": [
        "Vocabulary can be saved in a new file called vocab.txt which is later loaded and used to filter the movie reviews prior to encoding and also for the modelling defining a new function called save_list() which saves the vocabulary to the file one word per line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5RgX8CZLUms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "# convert lines to a single blob of text\n",
        "data = '\\n'.join(lines)\n",
        "# open file\n",
        "file = open(filename, 'w')\n",
        "# write text\n",
        "file.write(data)\n",
        "# close file\n",
        "file.close()\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8eG_lsYLUmt",
        "colab_type": "text"
      },
      "source": [
        "Pulling all this together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnjWhabHLUmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "return tokens\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "doc = load_doc(filename)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc)\n",
        "# update counts\n",
        "vocab.update(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "add_doc_to_vocab(path, vocab)\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "# convert lines to a single blob of text\n",
        "data = '\\n'.join(lines)\n",
        "# open file\n",
        "file = open(filename, 'w')\n",
        "# write text\n",
        "file.write(data)\n",
        "# close file\n",
        "file.close()\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs('txt_sentoken/pos', vocab)\n",
        "process_docs('txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw0HT5EPLUmx",
        "colab_type": "text"
      },
      "source": [
        "# 15.3 Train CNN With Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1AzKnayLUmx",
        "colab_type": "text"
      },
      "source": [
        "Learning a word embedding while training a convolutional neural network for the classification as the word embedding is the only way representing the text where the each word in the vocabulary is represente by a real valued vector oon a high dimensional space and the vectors that are learned in a way that the words having the similar meanings and also the similar representations in the vector space and the relationships between the words and tokens are ignored or forced in bigram and trigram approaches.\n",
        "\n",
        "Real valued vector represents for the words that can be learned while training the neural network which can do the kind of keras deep learning library using the embedding layer as the first step is to load the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nuM2kl1LUmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0ha0-tCLUm0",
        "colab_type": "text"
      },
      "source": [
        "The updated clean_doc()function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcjngyYmLUm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# filter out tokens not in vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "tokens = ' '.join(tokens)\n",
        "return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQXxZEYgLUm4",
        "colab_type": "text"
      },
      "source": [
        "The updated process_docs()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKpvKsr4LUm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "documents = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load the doc\n",
        "doc = load_doc(path)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc, vocab)\n",
        "# add to list\n",
        "documents.append(tokens)\n",
        "return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWC_kICHLUm6",
        "colab_type": "text"
      },
      "source": [
        "As the process_docs function for the both negative as well as positive directories combining together into a single train by defining the class labels for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0pV4QvdLUm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "return docs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Ic9gS3LUm8",
        "colab_type": "text"
      },
      "source": [
        "Moving on to the next step the encode of the each document as a sequence of integers as the keras embedding layer requires the integer inputs where the integer maps to a single token that having a specific beginning of traning but in traning it became meaningful to the specified network.Create_tokenizer() function below is preparing a tokenizer for the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz-o8lYDLUm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS53rrclLUm_",
        "colab_type": "text"
      },
      "source": [
        "Mapping of the words to the integers that has been prepared and we can use it for encoding the reviews in the training dataset by calling the texts_to_sequences() functon on the tokenizer for that we need all documents are having the same length.Calling out the Keras function pad_sequnces() the pad of the sequences for a maximum length by adding 0 values on the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lsfJA1BLUm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-r6MT16LUnB",
        "colab_type": "text"
      },
      "source": [
        "Using the maximum length as a parameter to a function the integer encode the pad sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTCxP0VbLUnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "# integer encode\n",
        "encoded = tokenizer.texts_to_sequences(docs)\n",
        "# pad sequences\n",
        "padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "return padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlUJ_4hmLUnD",
        "colab_type": "text"
      },
      "source": [
        "Ready to define the neural network model while using the model embedding layer as the first hidden layer and the embedding layer requires the specification for the vocabulary size and the size of the real-valued vector space having the maximum length of the input documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV1urRChLUnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm_01Nq8LUnJ",
        "colab_type": "text"
      },
      "source": [
        "Using the 100 dimensional vector space but also trying the other values such as 50 ro 150 and the maximum document length was calculated above by max_length variable used during the padding a conservative CNN configuration is used with 32 filters (parallel fields for processing words) and a kernel size of 8 with a rectified linear (relu) activation function.This is followed by a pooling layer that reduces the output of the convolutional layer by half."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsJDw9DmLUnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "model.summary()\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeYMD6GaLUnN",
        "colab_type": "text"
      },
      "source": [
        "getting a better result with a different configuration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nDE87tzLUnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls3ofLdeLUnO",
        "colab_type": "text"
      },
      "source": [
        "After this model is fitted it was saved to a file name model.h5 for later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WQjAfziLUnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbyORR8VLUnR",
        "colab_type": "text"
      },
      "source": [
        "Putting all this together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFu09P_9LUnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# filter out tokens not in vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "tokens = ' '.join(tokens)\n",
        "return tokens\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "documents = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load the doc\n",
        "doc = load_doc(path)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc, vocab)\n",
        "# add to list\n",
        "documents.append(tokens)\n",
        "return documents\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "return docs, labels\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "# integer encode\n",
        "encoded = tokenizer.texts_to_sequences(docs)\n",
        "# pad sequences\n",
        "padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "return padded\n",
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "model.summary()\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "return mode\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# save the model\n",
        "model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GMOl-yLLUnT",
        "colab_type": "text"
      },
      "source": [
        "#  15.4 Evaluate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_588xqwxLUnT",
        "colab_type": "text"
      },
      "source": [
        "Evaluating the trained model and using it to make the predicitions on the new data by firstly built-in evaluate() function to estimating the skills of the model on both training and the test dataset as this requries the load and encode of both the training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5qFdI_5LUnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEl4h3VTLUnV",
        "colab_type": "text"
      },
      "source": [
        "we can load the model and we can evaluate it on the both datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBWDS7a0LUnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print('Train Accuracy: %f' % (acc*100))\n",
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiaXzvLzLUnX",
        "colab_type": "text"
      },
      "source": [
        "new data must be prepared using the same text encoding and encoding schemes as was used on the training dataset and the function below named predict_sentiment() will encoding the pad which was given a movie review text and return a prediction in terms of both the precentage and the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTNZB4AqLUnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "# clean review\n",
        "line = clean_doc(review, vocab)\n",
        "# encode and pad review\n",
        "padded = encode_docs(tokenizer, max_length, [line])\n",
        "# predict sentiment\n",
        "yhat = model.predict(padded, verbose=0)\n",
        "# retrieve predicted percentage and label\n",
        "percent_pos = yhat[0,0]\n",
        "if round(percent_pos) == 0:\n",
        "return (1-percent_pos), 'NEGATIVE'\n",
        "return percent_pos, 'POSITIVE'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI9tNMhRLUna",
        "colab_type": "text"
      },
      "source": [
        "Testing out the two ad hoc movie reviews, the complete code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ7LSbw-LUna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "file = open(filename, 'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()\n",
        "return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "# split into tokens by white space\n",
        "tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# filter out tokens not in vocab\n",
        "tokens = [w for w in tokens if w in vocab]\n",
        "tokens = ' '.join(tokens)\n",
        "return tokens\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "documents = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "if is_train and filename.startswith('cv9'):\n",
        "continue\n",
        "if not is_train and not filename.startswith('cv9'):\n",
        "continue\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load the doc\n",
        "doc = load_doc(path)\n",
        "# clean doc\n",
        "tokens = clean_doc(doc, vocab)\n",
        "# add to list\n",
        "documents.append(tokens)\n",
        "return documents\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
        "pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
        "docs = neg + pos\n",
        "# prepare labels\n",
        "labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "return docs, labels\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "return tokenizer\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "# integer encode\n",
        "encoded = tokenizer.texts_to_sequences(docs)\n",
        "# pad sequences\n",
        "padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "return padded\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "# clean review\n",
        "line = clean_doc(review, vocab)\n",
        "# encode and pad review\n",
        "padded = encode_docs(tokenizer, max_length, [line])\n",
        "# predict sentiment\n",
        "yhat = model.predict(padded, verbose=0)\n",
        "# retrieve predicted percentage and label\n",
        "percent_pos = yhat[0,0]\n",
        "if round(percent_pos) == 0:\n",
        "return (1-percent_pos), 'NEGATIVE'\n",
        "return percent_pos, 'POSITIVE'\n",
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print('Train Accuracy: %.2f' % (acc*100))\n",
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))\n",
        "# test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_DpRZcfLUnd",
        "colab_type": "text"
      },
      "source": [
        "# 15.5 Extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6xDM4tLUnd",
        "colab_type": "text"
      },
      "source": [
        "Data Cleaning : Exploring the better data cleaning perhaps leaving some puncutations in  tact or normalizing the contractions\n",
        "\n",
        "Truncated Sequences : Padding all the sequences upto a length for a longest sequence that might be a extreme if the longeset sequence is very different to all other reviews studying the distribution of a review lengths and the truncate reviews to a mean length.\n",
        "\n",
        "Truncated Vocabulary: Removing the infrequently occuring words and had a large vocabulary of more than 25,000 words and exploring the further reducing the size of the vocabulary and the effect on the model skill\n",
        "\n",
        "Filters and Kernel Size :  Number of filters and kernel sizes are more important to the model skill and those were not tuned exploring the tuning these two CNN parameters.\n",
        "\n",
        "Epochs and Batch Size : As the model appears to be fit in the training dataset quickly exploring the alternate configurations of the number of training epochs and the batch size which are using the test dataset as a validation set for picking a better stop point for training the model.\n",
        "\n",
        "Deeper Network : Explore whether a deeper network results in better skill, either in terms of CNN layers, MLP layers and both.\n",
        "\n",
        "Pre-Train an Embedding : Explore pre-training a Word2Vec word embedding in the model and the impact on model skill with and without further fine tuning during training.\n",
        "\n",
        "Use GloVe Embedding : Explore loading the pre-trained GloVe embedding and the impact on model skill with and without further \fne tuning during training.\n",
        "\n",
        "Longer Test Reviews: Explore whether the skill of model predictions is dependent on the length of movie reviews as suspected in the \fnal section on evaluating the model.\n",
        "\n",
        "Train Final Model: Train a final model on all available data and use it make predictions on real ad hoc movie reviews from the internet."
      ]
    }
  ]
}