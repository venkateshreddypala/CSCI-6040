{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "How to Prepare Text Data with scikit-learn.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yysmWTaVKC_f",
        "colab_type": "text"
      },
      "source": [
        "# How to Prepare Text Data with Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMAaw5mTKC_h",
        "colab_type": "text"
      },
      "source": [
        "As the text ddata requires a special preparation before we start using it for prodicitive modeling as the text must be parsed to remove words called tokenization, then the words need to be enclosed as integers or floating point vales for use as inut to a machine learning algorithm called as feature extraction. As the scikit learn library offers easy-to-use tools to perform both tokenization and feature extraction of the text data for a predective modeling in the python language with the help of the scikit-learn \n",
        "\n",
        "How Axela, Google Assistant and Google Home can understand what we(humans) are saying or asking anything to them? How the first Robot citizen understands and speaks English with humans? It all starts with Natural language processing. From the last couple of years we humans have generated a huge amount of text data, and it is very important for machines to understand and process text data if you are want to give them the power of Artificial intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iN5TLxiKC_i",
        "colab_type": "text"
      },
      "source": [
        "# 6.1 The Bag-of-Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjHfFSPLKC_j",
        "colab_type": "text"
      },
      "source": [
        "While using the machine learning algorithms we cannot use the text directly, instead of that we need to convert the the whole text into numbers as we want to perform the classifications for the documents, so for each document is an iput and a class label is the output for our predictive algorithm.This algorithms takes vectors of numbers as input and therfore it convert the documents to a fixed length vectors of numbers.\n",
        "\n",
        "in bag-of-words model a simple and effective model for thinking about text document in machine learning.This model is simple in that it throes aways all the oder information in the words and focuses on the occurrence in words in a document,as this can be done by assingning each word a unqiue number,therefore any document we see can encoded as a fixed length vectr could be filled with a count or frequency of each word in the encoded document.\n",
        "\n",
        "As Machines can not understand English or any text data by default. The text data needs a special preparation before you can give text data to the machine to predict something out of it. That special preparation includes several steps such as removing stops words, correcting spelling mistakes, removing meaningless words, removing rare words.\n",
        "\n",
        "For encoding schemes the BWM (Bag-of-words-model) is only concerned that which represent what words are present or the degree to which they are present in encoded documents without any information about the order. This scikit=learn library provides totally 3 different schemes that can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQBxgGVfKC_j",
        "colab_type": "text"
      },
      "source": [
        "# 6.2 Word Counts with Count Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UApyKiDrKC_k",
        "colab_type": "text"
      },
      "source": [
        "Create an instance of the CountVectorizer class. Call the fit() function in order to learn a vocabulary from one or more documents. Call the transform() function on one or more documents as needed to encode each as a vector.\n",
        "\n",
        "As this Count Vectorizer provides a simple way for the both tokenize a collection of the text documents and to bulid a vocabulary of known words, but this also helps to encode new documents using the Vocabulary\n",
        "\n",
        "Using the Count Vectorizer in the following ways\n",
        "\n",
        "Create an instance of the Count Vectorizer class.\n",
        "\n",
        "Call the fit() function in order to learn a vocabulary from one or more documents.\n",
        "\n",
        "Call the transform() function on one or more documents as needed to encode each as a vector.\n",
        "\n",
        "When a encoded vector is returned with a lenght of the entire vocabulary and an integer count for number of times each word appeared in the paticular document.As the python language provides an efficient way of handling sparse vectors in the scipy.sparse package.The vectors that are returned from a call to transform() will be sparse vectors and those can transform them back to NumPy arrays to look and for a better understanding what is going on by calling the toarray() function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvRB0vHUKC_l",
        "colab_type": "code",
        "colab": {},
        "outputId": "edba7e4d-3c8e-48b2-9e5e-51b893cec9f2"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "#tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "#summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "#encode document\n",
        "vector = vectorizer.transform(text)\n",
        "#summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "(1, 8)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uos1gt9KC_p",
        "colab_type": "text"
      },
      "source": [
        "By default all the words were made by default and that the puncuations are ignored in other aspects the tokenizing can be configured to revie all options in the API documentation.So at last an array version of the encoded vector showing a count of 1st occurance for each word expect that has an 2nd occurance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G27ZyBMpKC_q",
        "colab_type": "code",
        "colab": {},
        "outputId": "616f8116-8331-4b9f-a644-3d2555a9f0d8"
      },
      "source": [
        "# encode another document\n",
        "text2 = [\"the puppy\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ie_PsdMKC_t",
        "colab_type": "text"
      },
      "source": [
        "# 6.3 Word Frequencies With Tfidfvectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woNvlKlXKC_t",
        "colab_type": "text"
      },
      "source": [
        "TFIDF stands for term frequency- inverse document frequency which means \n",
        "Term Frequency : For summarizing how often a given word appears within the document \n",
        "\n",
        "Inverse Document Frequency : It is about downscaling the words that appear a lot across the documents\n",
        "\n",
        "TFIDF is the product of the TF and IDF scores of the term.\n",
        "TF = number of times the term appears in the doc/total number of words in the doc\n",
        "IDF = ln(number of docs/number docs the term appears in)\n",
        "\n",
        "TFIDF is successfully used by search engines like Google, as a ranking factor for content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsPKCMBjKC_u",
        "colab_type": "code",
        "colab": {},
        "outputId": "35302a52-a342-4cb8-96ff-4bbc08f10e86"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "\"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "(1, 8)\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0Xm3cQgKC_w",
        "colab_type": "text"
      },
      "source": [
        "# 6.4 Hashing With HashingVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo2Mx_ODKC_x",
        "colab_type": "text"
      },
      "source": [
        "In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix.This HashingVectorizer class implements the approach which can be used in consistently hash words,in previous cases remembering back to computer science classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5KLUNrxKC_x",
        "colab_type": "code",
        "colab": {},
        "outputId": "1cc807b4-d74c-4edc-e48a-b8cb8e198295"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = HashingVectorizer(n_features=20)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 20)\n",
            "[[ 0.          0.          0.          0.          0.          0.33333333\n",
            "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
            "   0.          0.          0.         -0.33333333  0.          0.\n",
            "  -0.66666667  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}