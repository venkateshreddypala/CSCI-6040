{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bag_of_words.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWGMKZU4za5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkQi7vR6zfAg",
        "colab_type": "text"
      },
      "source": [
        "# Bag-Of- Words\n",
        "\n",
        "\n",
        "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR).The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.In this a very common feature extraction procedures for sentences and documents is the bag-of-words approach in this paticular approach as the histogram of the words within the text i.e considering each word count as a feature.\n",
        "\n",
        "\n",
        "\n",
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms. ... A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: A vocabulary of known words.\n",
        "\n",
        "\n",
        "The bag of words is a way of representing  fatures in the textual data.\n",
        "It invloves two things\n",
        "\n",
        "1. A vocabulary of known words.\n",
        "2. A measeure of presence of known words.\n",
        "\n",
        "In bag of words, vector x are derived from textual data, to represent the linguistic properties of a choosen language.\n",
        "This model is important in the folowing implementations\n",
        "1. Language Modelling \n",
        "2. Document Classifiaction\n",
        "\n",
        "This notebook hopefully will provide you enough knowledge for feature extraction in natural language processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcgyTGJA9Ab_",
        "colab_type": "text"
      },
      "source": [
        "**Procedure to build Bag-Of Words Model**\n",
        "\n",
        "Modeling text is that it is messy and techniques like machine learning algorithms prefer to a well defined fixed length inputs as well as outputs, as the machine learning algorithms cannot work with a raw text directly for that the text should be converted into numbers ,this type were called as feature extraction or feature encoding as this was a simple method for the featuring the extraction with the text data is known as Bag-of-Words model of text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Step 1: ColllectData: \n",
        "\n",
        "You can use project Gutenberg, for the raw data\n",
        "\n",
        "> Step 2: Design the Vocabulary:\n",
        "\n",
        "Create a list of Unique words ignoring the punctuation and case\n",
        "\n",
        "> Step 3: Create Document Vectors:\n",
        "\n",
        "This is to provide the score of words in each document\n",
        "\n",
        "> Step 4 : Managing Vocabulary\n",
        "\n",
        "We need to provide the concise library to aviod sparse vectors.\n",
        "\n",
        "> Step 4a: Implement pre-processing to manage the vocabulary, these were done in the previous notebooks\n",
        "\n",
        "a. Ingore the case.\n",
        "\n",
        "b. Ignore the punctuation.\n",
        "\n",
        "c. Ignore the frequent words(stop_words)\n",
        "\n",
        "d. Reducing the word to the stem.\n",
        "\n",
        "> Step 5: Scoring words:\n",
        "\n",
        "Once a vocabulary has been choosen, the occurance is scores, implement the Counts and the Frequencies.\n",
        "\n",
        "> Step 6: Word Hashing: \n",
        "\n",
        "This is also referred as Feature hashing, the challenge is to choose a hash space to accomodate the choosen vocabulary size to minimize the probability of collisions and trade-off sparsity.\n",
        "\n",
        "> Step 7: TF-IDF\n",
        "\n",
        "This is to keep a score for frequent words across all the documents, this helps to choose the important words across all the words.\n",
        "\n",
        "\n",
        "\n",
        "There is some more sophisticated approach for creating a vocabulary for the grouped words as this both change the scope for the vocabulary that is allowing the Bag-of-Words to capture meaning from the selected document this kind of word or token is well known as gram.For creating vocabulry for two-word pairs well called as bigram model as the bigrams appear in the corpus are mideled nt all possible bigrams.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9ZTz23Q2pIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "senten1= \"\"\"This is introduction to NLP & NLU\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rmhr9R23qb1",
        "colab_type": "code",
        "outputId": "b6e64c3c-fa51-4e7b-a178-830716ab1d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sentence_bow = {}\n",
        "for token in senten1.split(): sentence_bow[token] = 1\n",
        "sorted(sentence_bow.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('&', 1),\n",
              " ('NLP', 1),\n",
              " ('NLU', 1),\n",
              " ('This', 1),\n",
              " ('introduction', 1),\n",
              " ('is', 1),\n",
              " ('to', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vJYiPE84DUs",
        "colab_type": "text"
      },
      "source": [
        "This gives the words in the sentence and we can also call this model as word_frequency vector.\n",
        "\n",
        "As the Vocabulary been chosen the occurance of words in the selected documents needs to be scored.\n",
        "\n",
        "Counts: The no of times for count each words appears in the document\n",
        "\n",
        "Frequencies: Calculating the frequency for each word that appears in a document out of all the words that given the paticular document. \n",
        "\n",
        "Using a dictionary data type will remove the duplicates and makes the process much easy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDzTRh44frp",
        "colab_type": "code",
        "outputId": "82c95e2f-ef0d-4622-9ec6-464a17c73be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# implemeting dictionary\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in senten1.split()])), columns=['senten1']).T\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>This</th>\n",
              "      <th>is</th>\n",
              "      <th>introduction</th>\n",
              "      <th>to</th>\n",
              "      <th>NLP</th>\n",
              "      <th>&amp;</th>\n",
              "      <th>NLU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>senten1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         This  is  introduction  to  NLP  &  NLU\n",
              "senten1     1   1             1   1    1  1    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ido44g6p46Pa",
        "colab_type": "text"
      },
      "source": [
        "Converting the  data into a vector of features. When this is done using hashing we call the method \"feature hashing\" or \"the hashing trick\".Noticing that we just add 1 to the nth dimension of the vector each time our hash function returns that dimension for a word in the given text.\n",
        "\n",
        "For example let's our text be \n",
        "\n",
        "\"The quick white bunny\"\n",
        "As we represent the this as a vector the first and only thing that we are going to fix is the length of the vector and the no.of dimensions we are going to use for assumption let's take our using dimensions like be 5.Once we fix the number of dimensions we need a hash function that will take a string and return a number between 0 and n-1, in our case between 0 and 4. Any good hash function can be used and you just use h(string) mod n to make it return a number between 0 and n-1.\n",
        "\n",
        "\n",
        "Add more to the existing to sentences and perform the word of vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-3tJjpn3ueP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a play corpus\n",
        "senten1= \"\"\"This is introduction to NLP & NLU. \\n\"\"\"\n",
        "senten1 += \"\"\"Deep learning is the new electrcity. \\n\"\"\"\n",
        "senten1 += \"\"\"python is the best language! \\n\"\"\"\n",
        "senten1 += \"\"\"I like this note-book.\"\"\"\n",
        "corpus = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avF_7bTs3lOD",
        "colab_type": "text"
      },
      "source": [
        "In the case of  information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.As a IDF is one of the rare term is high,where as the idf pf a frequent term is likely to be low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cYPDr3v6W4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, sent in enumerate(senten1.split('\\n')): corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg-uyuj26iPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feFqU75q6qZm",
        "colab_type": "code",
        "outputId": "e1a2a411-0094-4364-afed-f972bb1e93c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "df[df.columns[:10]] "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&amp;</th>\n",
              "      <th>Deep</th>\n",
              "      <th>I</th>\n",
              "      <th>NLP</th>\n",
              "      <th>NLU.</th>\n",
              "      <th>This</th>\n",
              "      <th>best</th>\n",
              "      <th>electrcity.</th>\n",
              "      <th>introduction</th>\n",
              "      <th>is</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sent0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sent1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sent2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sent3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       &  Deep  I  NLP  NLU.  This  best  electrcity.  introduction  is\n",
              "sent0  1     0  0    1     1     1     0            0             1   1\n",
              "sent1  0     1  0    0     0     0     0            1             0   1\n",
              "sent2  0     0  0    0     0     0     1            0             0   1\n",
              "sent3  0     0  1    0     0     0     0            0             0   0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bot5UH27CZ9",
        "colab_type": "text"
      },
      "source": [
        "**Overlap in Bag-of-Words**\n",
        "If we measure the bag-of-words for two documents or two vectors, we can know the similarity between two documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kIvE2OR_MWw",
        "colab_type": "text"
      },
      "source": [
        "**Limitations**\n",
        "\n",
        "\n",
        "> Vocabulary\n",
        "\n",
        "Careful design is required to preserve the  word representations\n",
        "\n",
        "> Sparsity\n",
        "\n",
        "Sparse representations are hard to represent and the model takes much time to perform the computations\n",
        "\n",
        "> Meaning\n",
        "\n",
        "The meaning of the words is lost when the context is removed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2P8gFV-6s-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}